{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SuQuGti_OJm8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ga1g3p6oOJm9"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "sample_submission_path = 'data/sample_submission.csv'\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW3ZC-_ZOJm9"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def preprocess_and_engineer_features(df):\n",
    "    # Date and time features\n",
    "    df['trans_date'] = pd.to_datetime(df['trans_date'])\n",
    "    df['trans_year'] = df['trans_date'].dt.year\n",
    "    df['trans_month'] = df['trans_date'].dt.month\n",
    "    df['trans_day'] = df['trans_date'].dt.day\n",
    "    df['trans_weekday'] = df['trans_date'].dt.weekday\n",
    "    df['is_weekend'] = df['trans_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "    # Transaction time\n",
    "    df['trans_time'] = pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.hour * 3600 + \\\n",
    "                       pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.minute * 60 + \\\n",
    "                       pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.second\n",
    "    # Average Spend\n",
    "    df['average_spend'] = df.groupby('cc_num')['amt'].transform('mean')\n",
    "\n",
    "    # Recency spend ratio\n",
    "    # Calculate rolling average spend for each cardholder\n",
    "    df['recency_spend'] = df.groupby('cc_num')['amt'].rolling(window=10, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "    # Calculate the recency spend ratio\n",
    "    df['recency_spend_ratio'] = df['amt'] / df['recency_spend']\n",
    "\n",
    "    # average spend at a given merchant\n",
    "    df['merchant_average_spend'] = df.groupby('merchant')['amt'].transform('mean')\n",
    "\n",
    "    # Calculate distance between user and merchant\n",
    "    df['distance'] = np.sqrt((df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2)\n",
    "\n",
    "    # Log-transform transaction amount\n",
    "    df['log_amt'] = np.log1p(df['amt'])\n",
    "\n",
    "    # Ensure no missing values in cc_num\n",
    "    df['cc_num'] = df['cc_num'].fillna(-1)\n",
    "    df['cc_num'] = df['cc_num'].fillna(-1)\n",
    "\n",
    "    # Calculate percentage change in spending for each cardholder\n",
    "    df['spend_percentage_change'] = df.groupby('cc_num')['amt'].pct_change().fillna(0)\n",
    "    df['spend_percentage_change'] = df.groupby('cc_num')['amt'].pct_change().fillna(0)\n",
    "\n",
    "    # Extract transaction day\n",
    "    df['trans_day'] = pd.to_datetime(df['trans_date']).dt.date\n",
    "\n",
    "    # Calculate velocity (number of transactions per day per cardholder)\n",
    "    df['velocity'] = df.groupby(['cc_num', 'trans_day'])['trans_num'].transform('count')\n",
    "\n",
    "    # Extract age from DOB\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['age'] = (df['trans_date'] - df['dob']).dt.days // 365\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xDFJPgKLOJm9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"24579\" doesn't match format \"%H:%M:%S\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_engineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m preprocess_and_engineer_features(test_df)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mpreprocess_and_engineer_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_weekend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_weekday\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Transaction time\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrans_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     13\u001b[0m                    pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_time\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mminute \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     14\u001b[0m                    pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_time\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39msecond\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Average Spend\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_spend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc_num\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Extra Credit\\env\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Extra Credit\\env\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Extra Credit\\env\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"24579\" doesn't match format \"%H:%M:%S\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Preprocess datasets\n",
    "train_df = preprocess_and_engineer_features(train_df)\n",
    "test_df = preprocess_and_engineer_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCZ0vYxcOJm9"
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# Define the target column as 'is_fraud'\n",
    "# Drop unnecessary columns from train and test datasets to keep only relevant features\n",
    "target = 'is_fraud'\n",
    "features = train_df.drop(columns=['id', 'trans_num', 'trans_date', 'recency_spend_ratio', 'average_spend','distance', 'spend_percentage_change','velocity', 'trans_time', 'dob', 'is_fraud'])\n",
    "test_features = test_df.drop(columns=['id', 'trans_num', 'trans_date','recency_spend_ratio', 'average_spend', 'distance', 'spend_percentage_change', 'velocity', 'trans_time', 'dob'])\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "# Identify numerical columns as those with data types 'float64' or 'int64'\n",
    "# Identify categorical columns as those with the data type 'object'\n",
    "numerical_cols = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numerical features\n",
    "# Apply mean imputation for missing values and scale the data using StandardScaler\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "# Define preprocessing for categorical features\n",
    "# Apply most frequent imputation for missing values and one-hot encoding to handle categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors into a ColumnTransformer\n",
    "# Apply the numerical transformer to numerical columns and the categorical transformer to categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "# Define X as the feature set and y as the target for the training data\n",
    "# Define X_test as the preprocessed test feature set\n",
    "X = features\n",
    "y = train_df[target]\n",
    "X_test = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kg7ek0B8OJm9",
    "outputId": "3515ce57-51fc-435d-a7b9-4fc5291797d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.9727\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the preprocessor\n",
    "# Fit the preprocessor on the training data and transform both training and validation sets\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_val = preprocessor.transform(X_val)\n",
    "# Transform the test data using the same preprocessor\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "# originally was using random forest but I realized it was much slower and \n",
    "# less accurate to XGBClassifier\n",
    "model = XGBClassifier(n_estimators=1000, max_depth=20, scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), learning_rate=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "# Predict on the test dataset\n",
    "# Predict the validation set labels and calculate the F1-Score for validation\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Create the submission file\n",
    "# Copy the sample submission file and add predictions to the 'is_fraud' column\n",
    "submission = sample_submission.copy()\n",
    "submission['is_fraud'] = test_predictions\n",
    "# Save the submission file as 'submission.csv'\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Confirm successful creation of the submission file\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
